{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPt8CmCCENjzWA+cTkcb9Wg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fwW_h3--W-R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WorkFlow 1: Load WebSite content into Vector DB\n",
        "\n",
        "1.   Use LangChain LCEL\n",
        "2.  Prompting + LCEL + Output Parser\n",
        "3.  RAG (build once, re-use) with sources\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iYowg1xsngR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Map Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TwQsd2wftLHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "TdCPPOjJtUKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%pip install --upgrade jupyter-client"
      ],
      "metadata": {
        "id": "OFaZyp7BteR9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU \\\n",
        "    \"requests\" \\\n",
        "    \"langchain\" \\\n",
        "    \"langchain-openai\" \\\n",
        "    \"langchain-community\" \\\n",
        "    \"langchain-text-splitters\" \\\n",
        "    beautifulsoup4 lxml faiss-cpu langchainhub tavily-python \"gradio\""
      ],
      "metadata": {
        "id": "7GveVCsqt_UJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "def _ver(name):\n",
        "    try:\n",
        "        m = importlib.import_module(name)\n",
        "        return getattr(m, \"__version__\", \"n/a\")\n",
        "    except Exception as e:\n",
        "        return f\"not installed ({e})\"\n",
        "print(\"langchain           :\", _ver(\"langchain\"))\n",
        "print(\"langgraph           :\", _ver(\"langgraph\"))\n",
        "print(\"langchain-core      :\", _ver(\"langchain_core\"))\n",
        "print(\"langchain-community :\", _ver(\"langchain_community\"))\n",
        "print(\"langchain-openai    :\", _ver(\"langchain_openai\"))\n",
        "print(\"langchainhub        :\", _ver(\"langchainhub\"))\n",
        "print(\"langchain-text-splitters:\", _ver(\"langchain_text_splitters\"))\n",
        "print(\"faiss-cpu           :\", _ver(\"faiss\"))\n",
        "print(\"tavily-python       :\", _ver(\"tavily\"))"
      ],
      "metadata": {
        "id": "DwvCJf9EwFAq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "RoRGld2XxylA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load URL content"
      ],
      "metadata": {
        "id": "lcsNlNpZ0IvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1) Load docs (pick any public pages you want indexed)\n",
        "urls = [\n",
        "    \"https://www.apple.com/\",\n",
        "    \"https://www.apple.com/iphone/\",\n",
        "  # \"https://www.apple.com/ipad/\",\n",
        "  # \"https://www.apple.com/watch/\",\n",
        "    \"https://www.apple.com/mac/\"\n",
        "]"
      ],
      "metadata": {
        "id": "F5ZWKqtvyewc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "def load_and_summarize(url: str) -> Document:\n",
        "    \"\"\"\n",
        "    Loads an Apple.com page, extracts text, summarizes it,\n",
        "    and returns a LangChain Document safe for RAG.\n",
        "    \"\"\"\n",
        "    loader = WebBaseLoader(url)\n",
        "    docs = loader.load()\n",
        "\n",
        "    raw_text = docs[0].page_content\n",
        "\n",
        "    summary = llm.invoke(\n",
        "        f\"Summarize the key product facts from this Apple page. \"\n",
        "        f\"Focus on specs, features, and model differences.\\n\\n{raw_text}\"\n",
        "    )\n",
        "\n",
        "    return Document(\n",
        "        page_content=summary.content,\n",
        "        metadata={\"source\": url}\n",
        "    )\n"
      ],
      "metadata": {
        "id": "KKEAJXx50WpV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [load_and_summarize(url) for url in urls]\n"
      ],
      "metadata": {
        "id": "ds00sgoX0pS7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "vectorstore.save_local(\"apple_products\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gLySxt-fPFJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "vectorstore = FAISS.load_local(\n",
        "    \"apple_products\",\n",
        "    embeddings,\n",
        "    allow_dangerous_deserialization=True\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
      ],
      "metadata": {
        "id": "EAbxlQjQEg6Y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = retriever.invoke(\"best MacBook for video editing\")\n",
        "\n",
        "for r in results:\n",
        "    print(r.metadata[\"source\"])\n",
        "    print(r.page_content[:300], \"\\n\")"
      ],
      "metadata": {
        "id": "RsWZf5KkKGNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embeddings\n",
        "Generate Open AI embeddings, store in in-memory database and create retreiver object for similarity search"
      ],
      "metadata": {
        "id": "lVILHyMh3LAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate;\n",
        "system_prompt = \"\"\"\n",
        "You are \"Apple Sales Agent\", an expert Apple product specialist.\n",
        "\n",
        "Use a ReAct-style reasoning process INTERNALLY:\n",
        "- Thought: your internal reasoning about what to do next.\n",
        "- Action: the tool name and JSON arguments you want to call.\n",
        "- Observation: the result returned by the tool.\n",
        "- Answer: the final response you will give to the user.\n",
        "\n",
        "The user must NEVER see Thought, Action, or Observation.\n",
        "They ONLY see the final Answer.\n",
        "\n",
        "Tools you can call:\n",
        "\n",
        "- rag_product_search(query: str)\n",
        "  Use this when you need detailed product information from the product knowledge base.\n",
        "  It returns an array of chunks with product_id, title, content, and source.\n",
        "\n",
        "When using rag_product_search:\n",
        "- Craft a focused query that includes product family, use case, and key constraints.\n",
        "- Read the returned chunks carefully and base your Answer only on reliable information.\n",
        "- If information is missing or unclear, say you don’t know rather than inventing details.\n",
        "\n",
        "Your goals:\n",
        "1. Understand the customer's needs, constraints, and context.\n",
        "2. Recommend the best Apple products, configurations, and accessories.\n",
        "3. Explain trade-offs clearly and concisely.\n",
        "4. Never fabricate specs, prices, or availability.\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9oKXmvGlVwdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LLM Call\n",
        "Query LLM using LCEL pipeline."
      ],
      "metadata": {
        "id": "C2jfxvLjYo-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "# 5) LCEL pipeline: {question} flows through; {context} is produced by retriever\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 6) Try it\n",
        "rag_chain.invoke(\"I want to buy an IPhone\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QQsph8axYT6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_product_search(query: str) -> str:\n",
        "    \"\"\"Retrieve product info from FAISS vector DB.\"\"\"\n",
        "    docs = rag_chain.invoke(\"I want to an Iphone\")   # <-- FIXED\n",
        "\n",
        "    out = []\n",
        "    for d in docs:\n",
        "        out.append({\n",
        "            \"product_id\": d.metadata.get(\"product_id\"),\n",
        "            \"title\": d.metadata.get(\"title\"),\n",
        "            \"content\": d.page_content,\n",
        "            \"source\": d.metadata.get(\"source\"),\n",
        "        })\n",
        "    return str(out)\n"
      ],
      "metadata": {
        "id": "mZvc3FB9WQUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import gradio as gr\n",
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "def _final_text(res):\n",
        "    if isinstance(res, AIMessage):\n",
        "        return res.content or \"\"\n",
        "    if isinstance(res, dict) and \"messages\" in res:\n",
        "        for m in reversed(res[\"messages\"]):\n",
        "            if isinstance(m, AIMessage) or getattr(m, \"type\", \"\") == \"ai\":\n",
        "                return getattr(m, \"content\", \"\") or \"\"\n",
        "    return str(res)\n",
        "\n",
        "def _to_messages(history, message):\n",
        "    msgs = []\n",
        "    for u, a in history:\n",
        "        if u: msgs.append({\"role\": \"user\", \"content\": u})\n",
        "        if a: msgs.append({\"role\": \"assistant\", \"content\": a})\n",
        "    msgs.append({\"role\": \"user\", \"content\": message})\n",
        "    return msgs\n",
        "\n",
        "def _ensure_agent():\n",
        "    global agent\n",
        "    try:\n",
        "        agent\n",
        "        return agent\n",
        "    except NameError:\n",
        "        from langchain_openai import ChatOpenAI\n",
        "        from langchain.agents import create_agent  # Changed import\n",
        "        from langchain_core.tools import tool\n",
        "\n",
        "        @tool\n",
        "        def add(a: float, b: float) -> float:\n",
        "            \"Add two numbers.\"\n",
        "            return a + b\n",
        "\n",
        "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "        agent = create_agent(llm, [add], system_prompt=\"You are helpful.\")  # Changed parameter\n",
        "        return agent\n",
        "\n",
        "def chat_fn(message, history):\n",
        "    try:\n",
        "        ag = _ensure_agent()\n",
        "        msgs = _to_messages(history, message)\n",
        "        res = ag.invoke({\"messages\": msgs})\n",
        "        return _final_text(res)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "try:\n",
        "    demo.close()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Apple Sales Agent Chat\")\n",
        "    gr.Markdown(\"Ask about your KB (kb_search) or general queries. Web search only if TAVILY_API_KEY is set.\")\n",
        "    gr.ChatInterface(chat_fn)\n",
        "    gr.Markdown('Tip: Try \"Where are tracing docs?\" or \"Multiply 3.5 and 4.\"')  # Fixed quotes\n",
        "\n",
        "demo.launch(share=False)"
      ],
      "metadata": {
        "id": "E5KR-JKHoVHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- ReAct Agent Setup (create_agent) ----------\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.2,\n",
        ")\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are \"Apple Sales Agent\", an expert Apple product specialist.\n",
        "\n",
        "Use a ReAct-style reasoning process INTERNALLY:\n",
        "- Thought: your internal reasoning about what to do next.\n",
        "- Action: the tool name and JSON arguments you want to call.\n",
        "- Observation: the result returned by the tool.\n",
        "- Answer: the final response you will give to the user.\n",
        "\n",
        "The user must NEVER see Thought, Action, or Observation.\n",
        "They ONLY see the final Answer.\n",
        "\n",
        "Tools you can call:\n",
        "\n",
        "- rag_product_search(query: str)\n",
        "  Use this when you need detailed product information from the product knowledge base.\n",
        "  It returns an array of chunks with product_id, title, content, and source.\n",
        "\n",
        "When using rag_product_search:\n",
        "- Craft a focused query that includes product family, use case, and key constraints.\n",
        "- Read the returned chunks carefully and base your Answer only on reliable information.\n",
        "- If information is missing or unclear, say you don’t know rather than inventing details.\n",
        "\n",
        "Your goals:\n",
        "1. Understand the customer's needs, constraints, and context.\n",
        "2. Recommend the best Apple products, configurations, and accessories.\n",
        "3. Explain trade-offs clearly and concisely.\n",
        "4. Never fabricate specs, prices, or availability.\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "tools = [rag_product_search]\n",
        "\n",
        "agent = create_agent(\n",
        "    llm=llm,\n",
        "    tools=tools,\n",
        "    prompt=prompt,\n",
        ")\n",
        "\n",
        "# ---------- Public API for your app / UI ----------\n",
        "\n",
        "def run_sales_agent(user_message: str) -> str:\n",
        "    \"\"\"\n",
        "    Invoke the ReAct-style LangChain agent and return only the final answer.\n",
        "    \"\"\"\n",
        "    result = agent.invoke({\"input\": user_message})\n",
        "    # LangChain's create_agent returns a dict-like structure with \"output\"\n",
        "    return result.get(\"output\", str(result))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Quick manual test\n",
        "    msg = \"I need a MacBook for 4K video editing under $2500 with at least 1TB storage.\"\n",
        "    reply = run_sales_agent(msg)\n",
        "    print(\"Agent:\", reply)"
      ],
      "metadata": {
        "id": "pQ_s3QgbyWOo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}