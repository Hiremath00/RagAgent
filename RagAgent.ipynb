{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwTVgM35hreXx0dZbUXw+4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fwW_h3--W-R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WorkFlow 1: Load WebSite content into Vector DB\n",
        "\n",
        "1.   Use LangChain LCEL\n",
        "2.  Prompting + LCEL + Output Parser\n",
        "3.  RAG (build once, re-use) with sources\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iYowg1xsngR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Map Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TwQsd2wftLHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "TdCPPOjJtUKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade jupyter-client"
      ],
      "metadata": {
        "id": "OFaZyp7BteR9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU \\\n",
        "    \"requests\" \\\n",
        "    \"langchain\" \\\n",
        "    \"langchain-openai\" \\\n",
        "    \"langchain-community\" \\\n",
        "    \"langchain-text-splitters\" \\\n",
        "    beautifulsoup4 lxml faiss-cpu langchainhub tavily-python \"gradio\""
      ],
      "metadata": {
        "id": "7GveVCsqt_UJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "def _ver(name):\n",
        "    try:\n",
        "        m = importlib.import_module(name)\n",
        "        return getattr(m, \"__version__\", \"n/a\")\n",
        "    except Exception as e:\n",
        "        return f\"not installed ({e})\"\n",
        "print(\"langchain           :\", _ver(\"langchain\"))\n",
        "print(\"langgraph           :\", _ver(\"langgraph\"))\n",
        "print(\"langchain-core      :\", _ver(\"langchain_core\"))\n",
        "print(\"langchain-community :\", _ver(\"langchain_community\"))\n",
        "print(\"langchain-openai    :\", _ver(\"langchain_openai\"))\n",
        "print(\"langchainhub        :\", _ver(\"langchainhub\"))\n",
        "print(\"langchain-text-splitters:\", _ver(\"langchain_text_splitters\"))\n",
        "print(\"faiss-cpu           :\", _ver(\"faiss\"))\n",
        "print(\"tavily-python       :\", _ver(\"tavily\"))"
      ],
      "metadata": {
        "id": "DwvCJf9EwFAq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "RoRGld2XxylA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load URL content"
      ],
      "metadata": {
        "id": "lcsNlNpZ0IvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG (v1): Web loader → splitter → FAISS → retriever → LCEL chain\n",
        "#import os\n",
        "#os.environ.setdefault(\"USER_AGENT\", \"IK-LangChain-RAG/1.0 (contact: ops@your-org)\")  # fixes the warning\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1) Load docs (pick any public pages you want indexed)\n",
        "urls = [\n",
        "    \"https://python.langchain.com/docs/get_started/introduction/\",\n",
        "    \"https://docs.smith.langchain.com/\"\n",
        "]\n",
        "loader = WebBaseLoader(urls)\n",
        "docs = loader.load()\n",
        "print(docs)"
      ],
      "metadata": {
        "id": "F5ZWKqtvyewc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chunking\n",
        "Use LangChain Recursive Text Splitter for Chunking"
      ],
      "metadata": {
        "id": "jti8GeRK1HUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Chunk using LangChain recursive splitter\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "print(chunks)\n"
      ],
      "metadata": {
        "id": "7DaQfxk_1Psc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embeddings\n",
        "Generate Open AI embeddings, store in in-memory database and create retreiver object for similarity search"
      ],
      "metadata": {
        "id": "lVILHyMh3LAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Embed & index\n",
        "emb = OpenAIEmbeddings()  # uses OPENAI_API_KEY from env\n",
        "#vector database chromadb, FAISS , OPENSERACH ,\n",
        "vs = FAISS.from_documents(chunks, emb)\n",
        "retriever = vs.as_retriever(search_kwargs={\"k\": 4}) #return 4 semantically close results\n",
        "print(retriever)"
      ],
      "metadata": {
        "id": "A_KQPBxU3I1G",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate;\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"You are a precise assistant. Use the provided CONTEXT to answer.\\n\"\n",
        "     \"If the answer isn't in the context, say you don't know.\\n\\nCONTEXT:\\n{context}\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])"
      ],
      "metadata": {
        "id": "BQR2IoaI4W7P",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LLM Call\n",
        "Query LLM using LCEL pipeline."
      ],
      "metadata": {
        "id": "C2jfxvLjYo-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "# 5) LCEL pipeline: {question} flows through; {context} is produced by retriever\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 6) Try it\n",
        "rag_chain.invoke(\"What is LangSmith and how does it relate to LangChain?\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QQsph8axYT6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import gradio as gr\n",
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "def _final_text(res):\n",
        "    if isinstance(res, AIMessage):\n",
        "        return res.content or \"\"\n",
        "    if isinstance(res, dict) and \"messages\" in res:\n",
        "        for m in reversed(res[\"messages\"]):\n",
        "            if isinstance(m, AIMessage) or getattr(m, \"type\", \"\") == \"ai\":\n",
        "                return getattr(m, \"content\", \"\") or \"\"\n",
        "    return str(res)\n",
        "\n",
        "def _to_messages(history, message):\n",
        "    msgs = []\n",
        "    for u, a in history:\n",
        "        if u: msgs.append({\"role\": \"user\", \"content\": u})\n",
        "        if a: msgs.append({\"role\": \"assistant\", \"content\": a})\n",
        "    msgs.append({\"role\": \"user\", \"content\": message})\n",
        "    return msgs\n",
        "\n",
        "def _ensure_agent():\n",
        "    global agent\n",
        "    try:\n",
        "        agent\n",
        "        return agent\n",
        "    except NameError:\n",
        "        from langchain_openai import ChatOpenAI\n",
        "        from langchain.agents import create_agent  # Changed import\n",
        "        from langchain_core.tools import tool\n",
        "\n",
        "        @tool\n",
        "        def add(a: float, b: float) -> float:\n",
        "            \"Add two numbers.\"\n",
        "            return a + b\n",
        "\n",
        "        llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "        agent = create_agent(llm, [add], system_prompt=\"You are helpful.\")  # Changed parameter\n",
        "        return agent\n",
        "\n",
        "def chat_fn(message, history):\n",
        "    try:\n",
        "        ag = _ensure_agent()\n",
        "        msgs = _to_messages(history, message)\n",
        "        res = ag.invoke({\"messages\": msgs})\n",
        "        return _final_text(res)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "try:\n",
        "    demo.close()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# LangChain Agent Chat\")\n",
        "    gr.Markdown(\"Ask about your KB (kb_search) or general queries. Web search only if TAVILY_API_KEY is set.\")\n",
        "    gr.ChatInterface(chat_fn)\n",
        "    gr.Markdown('Tip: Try \"Where are tracing docs?\" or \"Multiply 3.5 and 4.\"')  # Fixed quotes\n",
        "\n",
        "demo.launch(share=False)"
      ],
      "metadata": {
        "id": "E5KR-JKHoVHM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}