{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMVil11piZlE08lS7qGQ4T"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fwW_h3--W-R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WorkFlow 1: Load WebSite content into Vector DB\n",
        "\n",
        "1.   Use LangChain LCEL\n",
        "2.  Prompting + LCEL + Output Parser\n",
        "3.  RAG (build once, re-use) with sources\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iYowg1xsngR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Map Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TwQsd2wftLHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "TdCPPOjJtUKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%pip install --upgrade jupyter-client"
      ],
      "metadata": {
        "id": "OFaZyp7BteR9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU \\\n",
        "    \"requests\" \\\n",
        "    \"langchain\" \\\n",
        "    \"langchain-openai\" \\\n",
        "    \"langchain-community\" \\\n",
        "    \"langchain-text-splitters\" \\\n",
        "    beautifulsoup4 lxml faiss-cpu langchainhub tavily-python \"gradio\""
      ],
      "metadata": {
        "id": "7GveVCsqt_UJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "def _ver(name):\n",
        "    try:\n",
        "        m = importlib.import_module(name)\n",
        "        return getattr(m, \"__version__\", \"n/a\")\n",
        "    except Exception as e:\n",
        "        return f\"not installed ({e})\"\n",
        "print(\"langchain           :\", _ver(\"langchain\"))\n",
        "print(\"langgraph           :\", _ver(\"langgraph\"))\n",
        "print(\"langchain-core      :\", _ver(\"langchain_core\"))\n",
        "print(\"langchain-community :\", _ver(\"langchain_community\"))\n",
        "print(\"langchain-openai    :\", _ver(\"langchain_openai\"))\n",
        "print(\"langchainhub        :\", _ver(\"langchainhub\"))\n",
        "print(\"langchain-text-splitters:\", _ver(\"langchain_text_splitters\"))\n",
        "print(\"faiss-cpu           :\", _ver(\"faiss\"))\n",
        "print(\"tavily-python       :\", _ver(\"tavily\"))"
      ],
      "metadata": {
        "id": "DwvCJf9EwFAq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "RoRGld2XxylA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load URL content"
      ],
      "metadata": {
        "id": "lcsNlNpZ0IvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "import bs4\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "# 1) Load docs (pick any public pages you want indexed)\n",
        "urls = [\n",
        "    \"https://www.apple.com/\",\n",
        "    \"https://www.apple.com/iphone/\",\n",
        "  # \"https://www.apple.com/ipad/\",\n",
        "  # \"https://www.apple.com/watch/\",\n",
        "    \"https://www.apple.com/mac/\"\n",
        "]"
      ],
      "metadata": {
        "id": "F5ZWKqtvyewc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "loader = WebBaseLoader(web_paths=urls, bs_kwargs=dict(\n",
        "                parse_only=bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "      ),\n",
        "    )\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
        "                                               chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "retriever = vectorstore.as_retriever()\n"
      ],
      "metadata": {
        "id": "KKEAJXx50WpV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_classic.chains import create_history_aware_retriever  # as of 20251214, using classic -> this is an open issue with standard langchain library https://github.com/langchain-ai/langchain-community/issues/433\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, \"\n",
        "    \"just reformulate it if needed and otherwise return it as is.\"\n",
        ")\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", contextualize_q_system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# 4. Create the history-aware retriever chain\n",
        "# This chain takes the current input and chat history, generates a standalone question,\n",
        "# and passes it to the base retriever to fetch relevant documents.\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    prompt=contextualize_q_prompt\n",
        ")"
      ],
      "metadata": {
        "id": "mwQ-JzItH6z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_classic.chains import create_retrieval_chain  # as of 20251214, using classic -> this is an open issue with standard langchain library https://github.com/langchain-ai/langchain-community/issues/433\n",
        "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "system_prompt_text = \"\"\"\n",
        "You are \"Apple Sales Agent\", an expert Apple product specialist.\n",
        "\n",
        "Use a ReAct-style reasoning process INTERNALLY:\n",
        "- Thought: your internal reasoning about what to do next.\n",
        "- Action: the tool name and JSON arguments you want to call.\n",
        "- Observation: the result returned by the tool.\n",
        "- Answer: the final response you will give to the user.\n",
        "\n",
        "The user must NEVER see Thought, Action, or Observation.\n",
        "They ONLY see the final Answer.\n",
        "\n",
        "Tools you can call:\n",
        "\n",
        "- rag_product_search(query: str)\n",
        "  Use this when you need detailed product information from the product knowledge base.\n",
        "  It returns an array of chunks with product_id, title, content, and source.\n",
        "\n",
        "When using rag_product_search:\n",
        "- Craft a focused query that includes product family, use case, and key constraints.\n",
        "- Read the returned chunks carefully and base your Answer only on reliable information.\n",
        "- If information is missing or unclear, say you donâ€™t know rather than inventing details.\n",
        "\n",
        "Your goals:\n",
        "1. Understand the customer's needs, constraints, and context.\n",
        "2. Recommend the best Apple products, configurations, and accessories.\n",
        "3. Explain trade-offs clearly and concisely.\n",
        "4. Never fabricate specs, prices, or availability.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "system_prompt = (\n",
        "system_prompt_text +\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "# history_aware_retriever and question_answer_chain in sequence, retaining\n",
        "# intermediate outputs such as the retrieved context for convenience.\n",
        "# It has input keys input and chat_history, and includes input, chat_history,\n",
        "# context, and answer in its output.\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever,\n",
        "                                   question_answer_chain)\n",
        "#  contextualize_q_prompt | llm1 | retriver | qa_prpmt | llm2"
      ],
      "metadata": {
        "id": "sBx3W1h_Li_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"I want to purchase an iPhone\"\n",
        "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
        "print(ai_msg_1[\"answer\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "6VevATiOMlCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "chat_history.extend(\n",
        "    [\n",
        "        HumanMessage(content=question),\n",
        "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "second_question = \"What are the models available?\" # What are common ways of doing task decompsition\n",
        "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
        "\n",
        "print(ai_msg_2[\"answer\"])"
      ],
      "metadata": {
        "id": "yC26OD7BM2nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chat_history.extend(\n",
        "    [\n",
        "        HumanMessage(content=question),\n",
        "        AIMessage(content=ai_msg_2[\"answer\"]),\n",
        "    ]\n",
        ")\n",
        "third_question = \"What is the price of the oldest one\"\n",
        "ai_msg_3 = rag_chain.invoke({\"input\": third_question, \"chat_history\": chat_history})\n",
        "\n",
        "print(ai_msg_3[\"answer\"])"
      ],
      "metadata": {
        "id": "UCstI57KN7KC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}